<ul>
	<li><i>algorithm</i>: a recipe for computing something. Programs are instances of an algorithm, like you're an instance of the human genome.

	<br><br>When people talk about "ML algorithms" they tend to mean <i>learners</i>. <a href="#fn:404" id="fnref:404">404</a></li><br>
	
	<li><i>machine learning</i>: "LEARNING = REPRESENTATION + EVALUATION + OPTIMIZATION." <a href="#fn:405" id="fnref:405">405</a></li>

	<li><i>learner</i>: an inductive program that takes in examples and outputs a model.</li>
	<li><i>model</i>: a program that predicts outputs from examples. (see also <i>classifier</i>, <i>regressor</i>).</li>
	<li><i>data scientist</i>: an inductive program that takes a vaguely posed problem, plus a dataset, and outputs a learner.</li><br>

	<li><i>examples</i>: rows. (AKA record, case, instance, observation, data point, 'sample' <a href="#fn:403" id="fnref:403">403</a>.)In most usecases, examples are tuples of scalars.</li>
	
	<li><i>features</i>: columns. (AKA attributes, explanatory variables, independent variables, predictors, manipulated variables, inputs, exposure, risk factors, regressors, variates, covariates.)</li>
	<li><i>output</i>: (AKA response variable, dependent variable, predicted, explained, experimental, outcome, regressand, label.)</li>
	<li><i>index</i>: row names</li>
	<li><i>schema</i> or <i>header</i>: column names</li><br>


	<li><i>training set</i>: the examples given to the learner</li>
	<li><i>test set</i>: the examples given to the model</li>
	<li><i>contamination</i>: using test data at any stage during building the model</li>
	<li><i>dimensionality</i>: the number of features you give to the learner.</li><br>

	<li><i>classifier</i>: a model that takes in numbers and gives you a type in response.</li>
	<li><i>regressor</i>: a model that takes in numbers and gives you a number in response.</li><br>

	<li><i>overfitting</i>: hallucinating a classifier; putting too much trust in flukes or mistakes in the training set. (<i>encoding the noise</i>) </li><br>

	<li><i>bias</i>: underfitting. a learner’s tendency to consistently learn the wrong thing.</li>
	<li><i>variance</i>: overfitting. a learner’s tendency to learn random things and not the real signal</li>
	<li><i>bias–variance tradeoff</i>: you can't both finding a function that generalises v finding a function that captures all the signal of the training set. Simple functions mean bias, complex functions mean variance.</li><br>

	<li><i>hypothesis space</i>: the set of classifiers that a learner can learn. Determined by representation choice (and by the optimisation function, if the evaluation function has multiple optimal choices).</li><br>

	<li><i>theory</i>: a machine that answers a class of questions. <a href="#fn:3" id="fnref:3">3</a></li>
</ul>

