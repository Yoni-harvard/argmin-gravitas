<ul>
	<li><b>algorithm</b>: a recipe for computing something. Programs are instances of an algorithm, like how are instances of .
	<br>When people talk about "ML algorithms" they tend to mean <i>learners</i>.</li><br>
	
	<li><b>learner</b>: an inductive program that takes in examples and outputs a model.</li>
	<li><b>model</b>: a program that predicts outputs from examples. (see also <i>classifier</i>, <i>regressor</i>).</li>
	<li><b>data scientist</b>: an inductive program that takes a vaguely posed problem, plus a dataset, and outputs a learner.</li><br>

	<li><b>examples</b>: rows</li>
	<li><b>features</b>: columns</li>
	<li><b>training set</b>: the examples given to the learner</li>
	<li><b>test set</b>: the examples given to the model</li>
	<li><b>contamination</b>: using test data at any stage during building the model</li>
	<li><b>dimensionality</b>: the number of features you give to the learner.</li><br>

	<li><b>classifier</b>: a model that takes in numbers and gives you a type in response.</li>
	<li><b>regressor</b>: a model that takes in numbers and gives you a number in response.</li><br>

	<li><b>overfitting</b>: hallucinating a classifier; putting too much trust in flukes or mistakes in the training set. (<i>encoding the noise</i>) </li><br>

	<li><b>bias</b>: underfitting. a learner’s tendency to consistently learn the wrong thing.</li>
	<li><b>variance</b>: overfitting. a learner’s tendency to learn random things and not the real signal</li>
	<li><b>bias–variance tradeoff</b>: you can't both finding a function that generalises v finding a function that captures all the signal of the training set. Simple functions mean bias, complex functions mean variance.</li><br>

	<li><b>hypothesis space</b>: the set of classifiers that a learner can learn. Determined by representation choice (and by the optimisation function, if the evaluation function has multiple optimal choices).</li><br>

	<li><b>theory</b>: a machine that answers a class of questions. <a href="#fn:3" id="fnref:3">3</a></li>
</ul>