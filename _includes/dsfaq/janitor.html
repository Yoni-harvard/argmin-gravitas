
<h3>What is big data?</h3>
<div>
    It's <i>like, so 2013</i>.<br><br>

    Practically speaking, 'big data' is any dataset which cannot fit in memory on an expensive computer. More often, 'big data' refers to the use of that data: i.e. it is cheap, mass surveillance of private citizens by businesses and governments.
</div>

<h3>Whoa, that doesn't sound disruptive or transformative at all!</h3>
<div>
    Oh it is - just not in your favour. A model is often dangerous if it is wrong, and sometimes dangerous if it is right.<br><br>

    <a href="https://mathbabe.org">Cathy O'Neil</a> writes persuasively about perverse decision systems <a href="#fn:301" id="fnref:301">301</a>; from her I distilled this list of warning signs:<br><br>

    <i>Opacity</i><br>
    <ul>
        <li>Is the subject aware they are being modelled?</li>
        <li>Is the subject aware of the model's outputs?</li>
        <li>Is the subject aware of the model's predictors and weights?</li>
        <li>Is the data the model uses open?</li>
        <li>Is it dynamic - does it update on its failed predictions?</li>
    </ul><br>

    <i>Scale</i>
    <ul>
        <li>Does the model make decisions about many thousands of people?</li>
        <li>Is the model famous enough to change incentives in its domain?</li>
        <li>Does the model cause vicious feedback loops?</li>
        <li>Does the model assign high-variance population estimates to individuals?</li>
    </ul><br>

    <i>Damage</i>
    <ul>
        <li>Does the model work against the subject's interests?</li>
        <li>If yes, does the model do so in the social interest?</li>
        <li>Is the model fully automated, i.e. does it make decisions as well as predictions?</li>
        <li>Does the model take into account things it shouldn't?</li>
        <li>Do its false positives do harm? Do its true positives?</li>
        <li>Is the harm of a false positive symmetric with the good of a true positive?</li>
    </ul> 
</div>


<h3>A Hippocratic Oath of Modeling</h3>
<div>
    After the financial meltdown in 2008, the ex-quant <a href="http://www.ederman.com/new/index.html">Emanuel Derman</a> came up with this:

    <ul>
        <li>I will remember that I didn’t make the world, and it doesn’t satisfy my equations.</li>
        <li>Though I will use models boldly to estimate value, I will not be overly impressed by mathematics.</li>
        <li>I will never sacrifice reality for elegance without explaining why I have done so. Nor will I give the people who use my model false comfort about its accuracy. Instead, I will make explicit its assumptions and oversights.</li>
        <li>I understand that my work may have enormous effects on society and the economy, many of them beyond my comprehension.</li>
    </ul><br>

    Precisely because it is a bit overheated, it could catch on.
</div>

<h3>Enough deep social implications: get to the boring bit</h3>
<div>The "data life cycle", which you might be lucky enough to control, depending on your organisation size:<br><br>
    <ol>
        <li>Data capture.   </li>

        <li>Data maintenance. Busywork: any processing that doesn't generate value itself (movement, integration, cleansing, enrichment, extract-transform-load). Also calculating derived values like (Net revenue = Gross revenue - fixed costs - variable costs x units). </li>

        <li>Data synthesis. To creation values via some inductive logic: predictions or inferences.</li>

        <li>Data usage. Use in object-level tasks.</li>

        <li>Data publication. To send data outside the org. Includes data breaches.</li>

        <li>Data archival. To backup and then remove data from production environments.</li>

        <li>Data purging. To remove every copy from the org.</li>
    </ol>
</div>

<h3>What do people mean by "unstructured" data?</h3>
<div>
    It's another surprisingly vague term. It means something like "data without a schema, or normalisation, or metadata, or labelling as a good or bad outcome". Anything you couldn't analyse on a spreadsheet. Text corpuses are a good example: the meaning is in there somewhere, but "structured" (nonprobabilistic) methods won't help you much.<br><br>

    Or: Structured data is <i>joined-up</i> data: relations are known, duplicates have been taken out, absolute timings are given, connections to other rows are graphed.
</div>


<h3>When do you <i>have</i> to structure data?</h3>
<div>
    When performance (network latency or model training speed or model accuracy) is more important than the cost.
</div>

<h3>What are the downsides of more data?</h3>
<div>
  <ul>
      <li>If you're just adding features but not rows, then you might be damaging your model with overfit and redundancy.</li>
      <li>If most values are zero then you have a sparsity problem. (Yes, "zero" or "N/A" is still data.)</li>
      <li>If it takes you above the 64GB sweet spot, so it doesn't fit in one computer's RAM, and doesn't gain much from parallelisation. Much harder to manipulate, even in these halcyon days of Spark and Sqoop.</li>
      <li>Some datasets are fantastically expensive, and you don't necessarily know that it will even help before you buy.</li>
  </ul>
</div>
