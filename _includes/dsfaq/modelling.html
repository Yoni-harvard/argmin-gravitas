
<h3>What's a model?</h3>
<div>
	A mathematical structure that approximates a part of the world. (Hopefully the part that caused your data.) Writing a model out in equations or programs can make it completely precise (and this allows us to computerise it). A model summarises, simplifies, unifies, and guides. Occasionally it surpasses.<br><br>

	Models are <i>intentionally</i> fake and smooth toys. This is because adding too much detail takes too much time and computing to be useful, and makes it impossible to make general claims, and actually prevents you from understanding the thing at hand. ("Can't see the wood for the trees.")<br><br> 

	A model's abstraction allows us to see the unity of seemingly unrelated problems (for instance, magnetisation, the changes in states of matter, and urban riots are all explained by just one kind of model, <a href="{{crit}}">criticality theory</a>). The brain of the C. elegans worm, the US power grid, and the film collaborations are all very well described by one kind of mathematical object, a "<a href="http://worrydream.com/refs/Watts-CollectiveDynamicsOfSmallWorldNetworks.pdf">small-world network</a>". And many phenomena fall under the same "power law" distribution and can be said to share a deep structure.

	<br><br>
	A <i>statistical</i> model is one which admits that it's not going to get the exact right answer every time (or any time) but which offers a good approximation to an uncertain world, by giving a weighted spread of possible values for a given input.<br><br> 

	In general, even when we call our model "highly predictive", we do not <i>predict</i> the future; rather we're reducing how wrong we are about the future.
</div>


<h3>Why does data leave us uncertain?</h3>
<div>Because<br>
	<ul>
		<li>it is always incomplete (small samples, few features, physical limits)</li>
		<li>it is usually an indirect reflection of the thing of interest. (proxies, latency)</li>
		<li>it is noisy (measurement error, data corruption, unknown processes)</li>
		<li>it always has some risk of being fabricated.</li>
		<li>it is often ambiguous.</li>
		<li>of the fundamental <a href="{{Induct}}">nature of inductive thought</a>: you can never be <i>sure</i> via sampling.</li>
	</ul>
</div>

<h3>When can we model?</h3>
<div>
	In a <a href="{{loMu}}">great paper</a> reflecting on the Great Recession, Lo and Mueller give a taxonomy of uncertainty (in the course of arguing that economists mistook themselves as having physicists' luck, a tractable domain:<br>

	<ol>
		<li>"<i>Complete certainty</i>". <i>You are in a Newtonian clockwork universe with no residuals, no observer effects, utterly stable parameters. So, given perfect information, you yield perfect predictions</i>.</li><br>

		<li>"<i>Risk without uncertainty</i>". You know a probability distribution for an exhaustive set of outcomes. No statistical inference needed. This is life in a hypothetical honest casino, where the rules are transparent and always followed.  This situation bears little resemblance to financial markets.</li><br>

		<li>"<i>Fully Reducible Uncertainty</i>". There is one probability distribution over a set of known outcomes, but parameters are unknown. Like an honest casino, but one in which the odds are not posted and must therefore be inferred from experience. In broader terms, fully reducible uncertainty describes a world in which a single model generates all outcomes, and this model is parameterized by a finite number of unknown parameters that do not change over time and which can be estimated with an arbitrary degree of precision given enough data. As sample size increases, classical inference brings this level down to level 2.</li>

		<li>"<i>Partially Reducible Uncertainty</i>". The distribution generating the data changes too frequently or is too complex to be estimated, or it consists in several nonperiodic regimes. Statistical inference cannot ever reduce this uncertainty to risk. Four sources:
		<blockquote>
				(1) stochastic or time-varying parameters that vary too frequently to be estimated accurately; <br>
				(2) nonlinearities too complex to be captured by existing models, techniques, and datasets; <br>
				(3) non-stationarities and non-ergodicities that render useless the Law of Large Numbers, Central Limit Theorem, and other methods of statistical inference and approximation; <br>
				and (4) the dependence on relevant but unknown and unknowable conditioning information.
		</blockquote>
		At this level of uncertainty, modeling philosophies and objectives in economics and finance begin to deviate significantly from those of the physical sciences... model-building in the social sciences should be much less informed by mathematical aesthetics, and much more by pragmatism in the face of partially reducible uncertainty. </i>
		 </li>
		
		<li>"<i>Irreducible uncertainty</i>". Ignorance so complete that it cannot be reduced using data: no distribution, so no success in risk management. Such uncertainty is beyond the reach of probabilistic reasoning, statistical inference, and any meaningful quantification. This type of uncertainty is the domain of philosophers and religious leaders, who focus on not only the unknown, but the unknowable.	</li>	
	</ol><br><br>

	We can model well when we are in situations 1 to 3; and we can model a bit, defensively, in situation 4.
</div>

<h3>What can modelling do?</h3>
<div>
	<ul>
		<li>summarise data</li>
		<li>predict new data</li>
		<li>simulate reality</li>
	</ul>
	Note that these three aims are actually super-sets of each other:<br><br>

	To predict new data is to use a model to summarise future data. (Since a model is also a compressed description of a dataset.) 

	<br><br>To simulate reality requires you to infer actual structure and true parameters: to infer these is to <i>predict</i> that future data will confirm your estimates of them, and the inference is also a prediction that repeat experiments will find the same parameters, if you factor out noise.


	<br><br>
	<div class="accordion">
		<h3>Model of modelling</h3>
		<div>
			<blockquote>
				All human activities can be described by five components: data, prediction, judgment, action, and outcomes. A visit to the doctor leads to:<br><br> 
				<b>1)</b> x-rays, blood tests, monitoring (data), <br>
				<b>2)</b> diagnosis: “if we administer treatment A, then we predict outcome X, but if we administer treatment B, then we predict outcome Y” (prediction), <br>
				<b>3)</b> “given your age, lifestyle, and family status, I think you might be best with treatment A...” (judgment);<br>
				<b>4)</b> administering treatment A (action), and <br>
				<b>5)</b> full recovery with minor side effects (outcome).<br><br>

				As machine intelligence improves, the value of human prediction skills will decrease because machine prediction will provide a cheaper and better substitute, just as machines did for arithmetic...
			</blockquote>
			<div align="center">- <a href="{{agrawal}}">Agrawal et al.</a></div>
			<br>
		</div>
	</div>

</div>


<h3>Why is my model wrong?</h3>
<div>Hoo boy.<br><br>

	First, recall that the "<i>error</i>" of model outputs denotes the uncertainty surrounding a number, not the blunders. Error in this sense is routine and non-blameworthy. There are two classes of error: <br><br>

	1. <u>Systematic uncertainties:</u>
	<ul>
		<li>are due to problems with calibrating the data collection process, or due to the modelling approach</li>
		<li>are usually correlated with previous measurements</li>
		<li>occur when a theory is not yet mature.</li>
	</ul>

	2. <u>Stochastic uncertainties</u> <a href="#fn:133" id="fnref:133">133</a>:
	<ul>
		<li>can arise from real fluctuations (e.g. random quantum processes),</li>
		<li>are uncorrelated with previous measurements,</li>
		<li>follow a well-developed theory;</li>
	</ul><br>
	We can split systematics into two, model error and parameter error <a href="#fn:134" id="fnref:134">134</a>:	

	<div class="accordion">
		<h3>Model error</h3>
		<div>
			<ul>
				<li>Model is approximation</li>
				<li>Best fit sucks</li>
				<li>Black swan</li>
				<li>Unlucky with training set: fits wrong model by chance.</li>
			</ul>
		</div>
		
		<h3>Parameter error</h3>
		<div>
			The incorrectness of the numbers that define the model, <i>given that the model is correct</i>. 

			<ul>
				<li><i>Concept drift</i>: the population values changed since you collected your data.</li>
				<li>Bad estimation</li>
				<ul>
					<li>via Sampling error</li>
					<li>via Systematic measurement error</li>
					<li>via numerical errors (in discretization, truncation, or round-off)</li>
				</ul>
			</ul>
			<br>Can be estimated by taking confidence intervals for each parameter; using bootstrapping to obtain your parameter estimates; or by Bayesian estimation of parameters, which price in such uncertainty automatically.
		</div>

		<h3>Stochastic error</h3>
		<div>
			Everything's fine, you just got unlucky.<br><br>

			Stochastic errors are neat and tidy: assumed to have a mean value of zero, to be uncorrelated with the predictor variables, to have constant variance, and to be uncorrelated with their own past values. Well-specified and easy to forecast (using simulation). 
		</div>
	</div><br>
</div>


<h3>My model used to be right; why is it wrong now?</h3>
<div>
	<ul>
		<li>It was overfitted to an imperfectly representative training set.</li>
		<li>The population has changed ("Concept drift")</li>
		<li>Your analysis environment has changed ("Data drift")</li>
	</ul><br>
	<div class="accordion">
		<h3>What causes data drift?</h3>
		<div>
			<ul><li>Structural drift: the source schema is changed</li>
			<li>Semantic drift: the data is constant but its meaning changes.</li>
			<li>Infrastructure drift: breaking change in an update to some part of pipeline.</li>
			</ul>
		</div>
	</div>
</div>
