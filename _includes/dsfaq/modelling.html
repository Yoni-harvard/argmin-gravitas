{%	assign Induct = "https://en.wikipedia.org/wiki/Problem_of_induction"		%}
{%	assign crit = "https://en.wikipedia.org/wiki/Critical_point_(thermodynamics)"		%}



<h3>What's a model?</h3>
<div>
	A mathematical structure that approximates a part of the world. (Hopefully the part that caused your data.) Writing a model out in equations or programs can make it completely precise (and this allows us to computerise it). A model summarises, simplifies, unifies, and guides. Occasionally it surpasses.<br><br>

	Models are <i>intentionally</i> fake and smooth toys. This is because adding too much detail takes too much time and computing to be useful, and makes it impossible to make general claims, and actually prevents you from understanding the thing at hand. ("Can't see the wood for the trees.")<br><br> 

	A model's abstraction allows us to see the unity of seemingly unrelated problems (for instance, magnetisation, the changes in states of matter, and riots are all explained by just one kind of model, <a href="{{crit}}">criticality theory</a>). And many phenomena fall under one "power law" distribution and can be said to share a deep structure.

	<br><br>
	A <i>statistical</i> model is one which admits that it's not going to get the exact right answer every time (or any time) but which offers a good approximation to an uncertain world, by giving a weighted spread of possible values for a given input.<br><br> 

	In general, even when we call our model "highly predictive", we do not <i>predict</i> the future; rather we're reducing how wrong we are about the future.
</div>


<h3>Why does data leave us uncertain?</h3>
<div>Because<br>
	<ul>
		<li>it is always incomplete (small samples, few features, physical limits)</li>
		<li>it is usually an indirect reflection of the thing of interest. (proxies, latency)</li>
		<li>it is noisy (measurement error, data corruption, unknown processes)</li>
		<li>it always has some risk of being fabricated.</li>
		<li>it is often ambiguous.</li>
		<li>of the fundamental <a href="{{Induct}}">nature of inductive thought</a>: you can never be <i>sure</i> via sampling.</li>
	</ul>
</div>


<h3>What can modelling do?</h3>
<div>
	<ul>
		<li>summarise data</li>
		<li>predict new data</li>
		<li>simulate reality</li>
	</ul>
	Note that these three aims are actually super-sets of each other:<br><br>

	To predict new data is to use a model to summarise future data. (Since a model is also a compressed description of a dataset.) 

	<br><br>To simulate reality requires you to infer actual structure and true parameters: to infer these is to <i>predict</i> that future data will confirm your estimates of them, and the inference is also a prediction that repeat experiments will find the same parameters, if you factor out noise.
</div>


<h3>Why is my model wrong?</h3>
<div>Hoo boy.<br><br>

	First, recall that the "<i>error</i>" of model outputs denotes the uncertainty surrounding a number, not the blunders. Error in this sense is routine and non-blameworthy. There are two classes of error: <br><br>

	1. <u>Systematic uncertainties:</u>
	<ul>
		<li>are due to problems with calibrating the data collection process, or due to the modelling approach</li>
		<li>are usually correlated with previous measurements</li>
		<li>occur when a theory is not yet mature.</li>
	</ul>

	2. <u>Stochastic uncertainties</u> <a href="#fn:133" id="fnref:133">133</a>:
	<ul>
		<li>can arise from real fluctuations (e.g. random quantum processes),</li>
		<li>are uncorrelated with previous measurements,</li>
		<li>follow a well-developed theory;</li>
	</ul><br>
	We can split systematics into two, model error and parameter error <a href="#fn:134" id="fnref:134">134</a>:	

	<div class="accordion">
		<h3>Model error</h3>
		<div>
			<ul>
				<li>Model is approximation</li>
				<li>Best fit sucks</li>
				<li>Black swan</li>
				<li>Unlucky with training set: fits wrong model by chance.</li>
			</ul>
		</div>
		
		<h3>Parameter error</h3>
		<div>
			The incorrectness of the numbers that define the model, <i>given that the model is correct</i>. 

			<ul>
				<li><i>Concept drift</i>: the population values changed since you collected your data.</li>
				<li>Bad estimation</li>
				<ul>
					<li>via Sampling error</li>
					<li>via Systematic measurement error</li>
					<li>via numerical errors (in discretization, truncation, or round-off)</li>
				</ul>
			</ul>
			<br>Can be estimated by taking confidence intervals for each parameter; using bootstrapping to obtain your parameter estimates; or by Bayesian estimation of parameters, which price in such uncertainty automatically.
		</div>

		<h3>Stochastic error</h3>
		<div>
			Everything's fine, you just got unlucky.<br><br>

			Stochastic errors are neat and tidy: assumed to have a mean value of zero, to be uncorrelated with the predictor variables, to have constant variance, and to be uncorrelated with their own past values. Well-specified and easy to forecast (using simulation). 
		</div>
	</div><br>
</div>


<h3>My model used to be right; why is it wrong now?</h3>
<div>
	<ul>
		<li>It was overfitted to an imperfectly representative training set.</li>
		<li>The population has changed ("Concept drift")</li>
		<li>Your analysis environment has changed ("Data drift")</li>
	</ul><br>
	<div class="accordion">
		<h3>What causes data drift?</h3>
		<div>
			<ul><li>Structural drift: the source schema is changed</li>
			<li>Semantic drift: the data is constant but its meaning changes.</li>
			<li>Infrastructure drift: breaking change in an update to some part of pipeline.</li>
			</ul>
		</div>
	</div>
</div>
