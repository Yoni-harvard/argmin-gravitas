
{%	assign elisseff = "http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf"		%}


<style type="text/css">
	td.rt {
  		text-align: right;
	}
</style>

<br>
The terminology hasn't settled down yet - the following terms are actually not disambiguated in common usage - but here's what a mature language might arrive at:<br>

<ul>
	<li><i>Feature engineering</i>: the manual design of inputs. Consists in
		<ol>
		<li><i>Feature generation</i>: finding all potentially relevant variables</li>
		<li><i>Feature selection</i>: choosing predictive features </li>
		<li><i>Feature construction</i>: deriving predictive, low-dimension, non-redundant features from variables with linear combinations, nonlinear transforms, etc.  <a href="#fn:401" id="fnref:401">401</a></li>
		</ol>
	</li>
</ul>
<br>

<div class="accordion">
	<h3>1. Feature generation</h3>
	<div>
		<br>This is a mixture of research and guesswork. We can use the following 3 bit code to classify your ideas for features, applied epistemology:

		<table>
		<tr>
			<td class="rt"></td><td class="rt">Relevant</td> <td class="rt">Measurable</td> <td class="rt">Considered</td>
		</tr>
		<tr>
			<td class="rt">Include these features</td>	<td class="rt">Y</td> 	<td class="rt">Y</td> 	<td class="rt">Y</td>
		</tr>
		<tr>
			<td class="rt">Drop these features</td>	<td class="rt">N</td> 	<td class="rt">Y</td> 	<td class="rt">Y</td>
		</tr>
		<tr>
			<td class="rt">Look for proxies</td>	<td class="rt"> Y</td> 	<td class="rt">N</td> 	<td class="rt">Y</td>
		</tr>
		<tr>
			<td class="rt">Whose idea was this?</td>	<td class="rt">N</td> 	<td class="rt">N</td> 	<td class="rt">Y</td>
		</tr>
		<tr>
			<td class="rt">Why we research</td>	<td class="rt">Y</td> 	<td class="rt">Y</td> 	<td class="rt">N</td>
		</tr>
		<tr>
			<td class="rt">Think about proxies<br>more</td>	<td class="rt">Y</td> 	<td class="rt">N</td> 	<td class="rt">N</td>
		</tr>
		<tr>
			<td class="rt">Averted waste of<br> time</td>	<td class="rt">N</td> 	<td class="rt">Y</td> 	<td class="rt">N</td>
		</tr>
		<tr>
			<td class="rt">Nobody cares</td>	<td class="rt">N</td> 	<td class="rt">N</td> 	<td class="rt">N</td>
		</tr>

		</table>

		<br><a href="#fn:402" id="fnref:402">402</a><br> 

	</div>

	<h3>2. Feature selection</h3>
	<div>
		<br>How to pick the best ones from your shortlist.<br><br>

			<div class="accordion">
				<h3>FS Checklist</h3>
				<div>
					from <a href="{{elisseff}}">Guyon and Elisseeff (2003)</a>:<br>

				<ol>
				<li><blockquote>Do you have domain knowledge? If yes, construct a better set of “ad hoc” features.</blockquote></li>
				
				<li><blockquote>Are your features commensurate? If no, consider normalizing them.</blockquote></li>

				<li><blockquote>Do you suspect interdependence of features? If yes, expand your feature set by constructing conjunctive features or products of features, as much as your computer resources allow you.</blockquote></li>

				<li><blockquote>Do you need to prune the input variables (e.g. for cost, speed or data understanding reasons)? If no, construct disjunctive features or weighted sums of features (e.g. by clustering or matrix factorization).</blockquote>
				</li>
				
				<li><blockquote>Do you need to assess features individually (e.g. to understand their influence on the system or because their number is so large that you need to do a first filtering)? If yes, use a variable ranking method; else, do it anyway to get baseline results.</blockquote></li>

				<li><blockquote>Do you need a predictor? If no, stop.</blockquote></li>

				<li><blockquote>Do you suspect your data is “dirty” (... has a few meaningless input patterns and/or noisy outputs or wrong class labels)? If yes, detect the outlier examples using the top ranking variables obtained in step 5 as representation; check and/or discard them.</blockquote></li>

				<li><blockquote>Do you know what to try first? If no, use a linear predictor. Use a forward selection method with the “probe” method as a stopping criterion or use the 0-norm embedded method. For comparison, following the ranking of step 5, construct a sequence of predictors of same nature using increasing subsets of features. Can you match or improve performance with a smaller subset? If yes, try a non-linear predictor with that subset.</blockquote></li>
				
				<li><blockquote>Do you have new ideas, time, computational resources, and enough examples? If yes, compare several feature selection methods, including your new idea, correlation coefficients, backward selection and embedded methods. Use linear and non-linear predictors. Select the best approach with model selection.</blockquote></li>

				<li><blockquote>Do you want a stable solution (to improve performance and/or understanding)? If yes, subsample your data and redo your analysis for several “bootstraps” .</blockquote></li>
				</ol>
				</div>


				<h3>Types of feature selection</h3>
				<div>
				<ol>
				<li>Filters. Univariate evaluation of possible features, followed by then ordering according to a criterion like RMS.
					<br>treating the features as independent. It thus returns correlated features. It thus gives no treatment of interactions.  
				</li>

				<li>
					Wrappers. Finds a subset of features. Of course, the possible subsets grow exponentially with set size. So there’s a nasty opportunity for overfitting.<br><br>
					1) pick an algorithm for selecting features. e.g. stepwise regression <br>
					2) pick a criterion or filter to decide if an output set of features is good.
				</li>
				</ol>
				</div>
			</div>
	</div>


	<h3>3. Feature construction</h3>
	<div>
		<br>

		<br> 
	</div>

</div>

