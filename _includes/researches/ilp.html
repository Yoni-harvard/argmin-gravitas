<tr>
	<td class="logo">
		<a href="{{ilp}}"><img src="/img/papers/ilp.png" /></a>
	</td>
	<td style="padding-left: 5px">
		<i><a href="{{ilp}}">Safety Properties of Inductive Logic Programming</a></i> (2020), AAAI SafeAI workshop,<br>
		1st author / 3. <br>
		<span class="dropdown">
		  <a class="dropped" href="javascript:drop('ilp')">The point</a>, 
		  <div id="ilp" class="dropdown-content">
			<br>
			We look at an obscure kind of machine learning, seeing if it is (or could be) safer than neural networks. We use an existing framework for thinking about AI safety, and formalise it a bit to allow the comparison.<br><br>
			Upsides: ILP is convenient for specification, is robust to some syntactic input changes, has greater control over the inductive bias, actually can be formally verified, and the results are pretty interpretable (you can read the model and see how it is built).<br><br>
			But ILP is (so far) limited to domains where you have nice neat symbolic data, it can't do architecture search, and its performance lags far behind NNs on almost all tasks. Hybrid systems of ILP and NNs look like they would lose most of what we like about ILP in the first place.<br><br>

			<i>Authors</i>: Gavin Leech, Nandi Schoots, Joar Skalse <br><br>
		  </div>
		</span>
		<a href="{{ilpvid}}">video</a>, <a href="/ilp">outtakes</a>
	</td>
</tr>