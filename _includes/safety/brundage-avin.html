Brundage, Avin, et al (2018) 'The Malicious Use of Artificial Intelligence'


<a href="https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf">Link.</a>

Classification:

	Domain: Policy;
	Timeframe: near-future;
	Mindset: Security
	Threat: Direct


    dual-use: double-edged tech
        artificial intelligence is dual-use in the same sense that human intelligence is. 
        Many tasks that it would be beneficial to automate are themselves dual-use. For example, systems that examine software for vulnerabilities have both offensive and defensive applications, and the difference between an autonomous drone used to deliver packages and the capabilities of an autonomous drone used to deliver explosives need not be very great. 
    
    
    Not lookingat indirect threats (mass unemployment) nor system-level threats: interaction between non-malicious actors, such as a “race to the bottom” on AI safety

    cybersecurity, drones, autonomous weapons, bots, terrorism.
    
    an AI system is “efficient” if, once trained, it can complete a certain task more quickly or cheaply than a human could.
    
    an AI system is “scalable” if, given that it can complete a certain task, increasing computing power or making copies would allow it to complete many more  instances of the task
    
    AI systems can exceed human capabilities.
    
    Increased anonymity and psychological distance.
        Using an autonomous weapons system, rather than a handgun, avoids both the need to be present at the scene and the need to look at their victim
    
    rapid diffusion
        access to software and relevant science. reproduced in a matter of days. high degree of openness, with many papers being accompanied by source code. difficult
    
    novel unresolved vulnerabilities. 
        data poisoning attacks 
        adversarial examples
        exploitation of flaws in the design of goals . 
        exceed human performance, but also fail in ways that a human never would
    
    What will change?
        More threats. Attack cost way down. Thus more actors committing each attack, higher rate, more targets. Cheaper and psych distant.
            Automation of social engineering
            Automation of vuln discovery
            Human-like denial-of-service
            Model extraction
        New threats. The otherwise impractical. Vulnerabilities of AI systems deployed by defenders. 
            worst-case scenario: attack on a server used to direct autonomous weapons, lead to large-scale friendly fire or civilian target
        Worse average threat. Effective, targeted, less attributable, adversarial. 
    e.g.
        Digital security. Automating cyberattacks obviates the tradeoff between scale and efficacy. Expand labor-intensive cyberattacks (such as spear phishing). Novel exploits vs humans (e.g. speech synthesis for impersonation), software (e.g. fuzzing), or AIs (e.g. adversarial examples, data poisoning). 
        Physical security. drones and other physical systems (e.g. through the deployment of autonomous weapons systems). novel attacks that subvert cyberphysical systems (e.g. causing autonomous vehicles to crash) infeasible to direct remotely (e.g. a swarm of thousands of micro-drones).
        Political security. surveillance (e.g. analysing mass-collected data), persuasion (e.g. creating targeted propaganda), and deception (e.g. manipulating videos). novel attacks that from improved analysis of behavior, moods, and beliefs on the basis of available data. authoritarian states, but may also undermine democracies. 
            Automated, hyper-personalised disinformation campaigns.
            Denial-of-information attacks
    Proposals
        Learning from cybersecurity. red teaming, formal verification, responsible disclosure of AI vulnerabilities, security tools, and secure hardware. 
        Openness models. need to reimagine the openness of research, starting with pre-publication risk assessment in technical areas of special concern, central access licensing models, sharing regimes that favor safety and security, and other lessons. 
        Promoting responsibility culture. Researchers can shape the security landscape of the AI-enabled world. education, ethical standards, framings, norms, and expectations.
        Tech and policy solutions. promising technologies, policy interventions: privacy protection, coordinated use of AI for public-good security, monitoring of AI-relevant resources, and legislative 