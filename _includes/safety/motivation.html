<h3>Why prioritise AI safety?</h3>

1. Improving the far future is what ultimately matters most.
2. Reducing existential risk and astronomical suffering risk is the best way for us to improve the far future.
3. AI safety is the best way to reduce X-risk.
4. ? is the best way to promote AI safety.


X-risk (and not a mugging)
Max scale, high neglectedness, low tractability
Value lock-in
Richard Hamming. Why aren't you working on them?

MIRI argument
    Advanced AI systems are going to have a huge impact on the world, 
    For many plausible systems, we won't be able to intervene after they become sufficiently capable.
    If we fundamentally "don't know what we're doing" because we don't have a satisfying description of how an AI system should reason and make decisions, then we will probably make lots of mistakes in the design of an advanced AI system.
    Even minor mistakes in an advanced AI system's design are likely to cause catastrophic misalignment.
    Because of 1, 2, and 3, if we don't have a satisfying description of how an AI system should reason and make decisions, we're likely to make enough mistakes to cause a catastrophe. The right way to get to advanced AI that does the right thing instead of causing catastrophes is to deeply understand what we're doing, starting with a satisfying description of how an AI system should reason and make decisions.
    This case does not revolve around any specific claims about specific potential failure modes, or their relationship to specific HRAD subproblems. This case revolves around the value of fundamental understanding for avoiding "unknown unknown" problems.

Scenarios
    1) Solving the AI safety problem will turn out to be unnecessary, and our fears today are founded on misunderstandings about the problem. My credence: 2%
    2) Solving the AI safety problem will turn out to be relative straightforward on the timeline available. My credence: 13%
    3) It will be a close call whether we manage to solve it in time - it will depend on how hard we work and when we start. My credence: 60%
    4) Solving the AI safety problem is almost impossible and we would have to be extremely lucky to do so before creating a super-intelligent machine. We are therefore probably screwed. My credence: 25%

    Kaufman on feedback
        We have enormous proofs which work (e.g. FLT). But they rarely work first time?
        Philae landing
        GridWorld