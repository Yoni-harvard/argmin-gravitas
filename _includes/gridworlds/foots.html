<div class="footnotes">
<ol>
    <li class="footnote" id="fn:1">
		See <a href="{{armstrong}}">Armstrong & Levinstein (2017)</a> for an approach via a vast explicit list of sentinel variables, or <a href="{{amodei}}">Amodei et al (2016)</a>'s impact regulariser. Future under policy vs null policy.
	</li>

	<li class="footnote" id="fn:2">
		<a href="{{miles}}">Idea from Robert Miles</a>.
	</li>

	<li class="footnote" id="fn:3">
		Formalising reversibility. See <a href="{{amodei}}">Amodei et al (2016)</a> on minimising 'empowerment' (the maximum possible mutual information between the agentâ€™s potential future actions and its potential future state) . <br><br>

	</li>

	<li class="footnote" id="fn:4">
		Reversibility regulariser. Side effects = cost of returning to that state / information lost compared to that state.
	</li>

	<li class="footnote" id="fn:5">
		
		<br><br>
		Tom's variant: adding human feedback before the calculation of the normalisation constant.
	</li>

<!-- 	<li class="footnote" id="fn:10">
		<code>for i in episodes :<br>
			if CIRL :<br>
				if i % 2 == 0 :<br>
					get_human_feedback()<br>
			if HUM_PREF <br>
				if i > 0 :<br>
					get_human_feedback()<br>
			if IRL :<br>
				if i < 10 :<br>
					get_human_feedback()<br>
		</code>

	</li> -->

</ol>
</div>



