<div class="accordion">
	<h3>Glossary</h3>
	<div>
	* RL: For an agent in an environment with unknown transition probabilities, find through trial and error the optimal policy. Given SARS samples from a reward function, get the optimal policy.

	* Inverse reinforcement learning: learning an agent's objectives, values, rewards by observing behavior. Promises to obviate handcrafting of reward functions.

	* Behavioral cloning: given trajectories, get optimal policy π∗.

	* IRL: given policy (/action history), get reward function. Treat actions as optimal. The reliable replication of a natural behaviour.

	* Trajectory ζ: A history. Ordered list of (state, action)s sampled from some policy.

	* environment: a Markov decision process
	
	* agent: MDP solver. part of the environment seeking to achieve something.
	
	* value function: V(s) is the total discounted long-term reward expected to be obtained by an agent following the policy and starting in state s.

	* expected value difference: proxy of recovered reward function fit.

	</div>
</div>
