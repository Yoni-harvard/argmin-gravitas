HMMs were originally used for automatic word recognition (i.e. for modelling dictionary words and outputting the most likely word or sentence given an input vector).<a href="#fn:41" id="fnref:41">41</a> Since we're focussed on speaker recognition instead, this bit covers the use of HMMs in speaker
verification, only introducing the statistical machinery in common. 

For example, in word-recognition applications, the point of multiple HMM states is to ensure that only certain patterns of phonemes
or words are accepted (representing the constraints of the morphology of the language, or its grammar). In our case, the states capture, instead, distributions of acoustic evidence over time: roughly, the speaker’s overall typical wave over the course of pronouncing a word.

A primary motivator for using HMMs in linguistics is that the models are doubly stochastic, i.e. they are
capable of modelling simultaneously two domains of speech, like time and frequency. 

The Markov PFSM is our representation of temporal changes in speech (for instance changes in speech rate or
intonation); the emission GMM of each state represents the variation in the spectral properties of a given linguistic token.

In general, you can use HMMs to model linguistic phenomena by building a recogniser for each desired
'phone' (isolated linguistic unit). This is shown in Fig.8. A recogniser consists in a trained set of 'phone
models', a series of unlabelled novel inputs (the test set) and some way of categorising the model’s
output according to the most likely unit (the transcriptions). The recogniser is, then, a map function
for mapping speech (represented as sequences of vectors) to symbol sequences.


<center>
<img src="/img/accommodation/8.png" width="40%" height="40%" />

<br><br>	
<i>Fig.8</i> - The general form of a HMM recogniser. Here, 3 HMMs are trained and checked. <br>
Agnostic about the entity being recognised (words, phonemes, speakers). <a href="#fn:42" id="fnref:42">42</a>
</center>
<br><br>	





This general recognition task is not easy. The mapping from linguistic symbols to speech waves is many-to-one; i.e. different symbols can easily give rise to similar speech sounds (consider
“foxhole” and “Vauxhall”, or “Lady Mondegreen” and “laid him on the green”). 

And the statistical noise between two waveform tokens of the same symbolic unit is often very great, due to variations
between speakers, between context (speaker mood, social cues) and paralinguistic effects (pitch,
intonation, speech rate). These problems make parameter optimisation and probabilistic judgment (e.g. offering multiple scored hypotheses) so key to recogniser performance.
<br><br>

<h4>Assumptions</h4>

We can only use HMMs for linguistic purposes given some key assumptions, including two important approximations to the speech signal:<br><br>

<ol>
	<li><i>Stationarity</i>: In order to have distinct training and testing phases for our models, we assume
that a speech signal can be well-approximated as a concatenation of static distributions
(more formally, as a ‘piecewise stationary process’, a stochastic process whose joint
probability distribution does not change over time). This requires us to first ‘window’ the
signal into discrete and periodic units, on the order of 10ms long: we assume stationary of
the waveform over this short duration (see section 5.2).</li><br>

	<li><i>Markov process</i>: that the sequence of observed speech vectors corresponding to
each word is generated by a Markov model (that each state depends only on the most
recent antecedent); and, as a corollary, that the information lost by making the Markov
assumption for speech does not occlude important patterns in the speech data, at least for
many stochastic purposes. (That is, we assume that the full, delicate N-gram chains of real
contextualised natural language are not the only property of interest.)</li><br>

	<li><i>Representativeness</i>: that the training data that will determine the model parameters is
properly representative of the linguistic unit being modelled.</li><br>

	<li><i>Unidirectionality</i>. That the only possible transitions are forward in time, since speech is a
temporal sequence. (i.e. Left-Right models.)</li>

</ol>
<br><br>
<hr />
<br><br>

<h4>4.1. HMMs for speaker verification</h4>
<br>
Using HMMs to authenticate "voice-prints" involves similar machinery to the
speech recognition case, but uses speakers as the ‘words’ to be recognised. The models can quite
reliably distinguish speakers through the use of characteristic differences between the speech of
different people (inter-speaker variability). The general speaker-verification problem is simply:

> Given a segment of speech, [$$O$$], and a hypothesized speaker, [$$A$$], the task of speaker detection... is to determine if [$$O$$] was spoken by [$$A$$].<a href="#fn:43" id="fnref:43">43</a>

<br>

A system for verifying speakers requires the following subsystems:<br><br>

<ol>
	<li>An <i>acoustic analysis</i> subsystem, which extracts many ‘features’ (vector measurements) from each signal at regular intervals (frames).</li><br>
	
	<li>A <i>modelling subsystem</i>, which creates a model of speaker \(A\) and estimates optimal parameters using training outputs from subsystem (1).</li><br>
	
	<li>A <i>scoring subsystem</i>, which determines how well each test output from (1) fits the model.</li><br>

	<li>A <i>decision subsystem</i>, which uses the output of (3) and a pre-specified precision threshold to authenticate or reject the speaker \(A\)	.</li>
</ol><br>

The acoustic analysis step is covered in section 5.2; the modelling is as outlined above in 3.2 (though actually, simple GMMs are the dominant approach for text-independent speaker verification). <a href="#fn:44" id="fnref:44">44</a>

One simple scoring measure is to evaluate the conditional probability (the likelihood) of an utterance
being generated by the model and to compare this . And a simple decision measure is the hypothesis
test between $$h_0$$ (“the utterance $$O_i$$ is not by speaker $$A$$”) and $$h_1$$ (“the utterance $$O_i$$ is by speaker $$A$$”):

$$
	v_i^{(A)} = \frac{ P(\, O_i^{(A)} \,|\ \theta_A) } { P(\, O_i^{(A)} \,|\, h_0	 ) }
	
	\qquad\qquad \text{(E25)}
$$

The ratio is then compared to a threshold value; if \(v_i^{(A)}\) exceeds it, \( O_i^{(A)} \) is accepted as an utterance by \(A\); if it does not, the null hypothesis is not rejected.

This approach – taking the likelihood ratio of an utterance, conditional on speaker models – was adapted for use in detecting accommodation by the SSSV experiment, on which the present work relies.<br><br><br>


<h4>4.2. HMMs for analysing accommodation</h4>

Finally, our particular method. In speaker verification modelling, each HMM is a series of statistical patterns representing the
‘acoustic realisation’ of a modelled speaker (i.e. a series of distributions summarising the speaker’s
characteristic speech vectors). Take the components of our speaker HMMs as representing the
following:<br><br>


<ul>
<li><i>The hidden process</i>: the non-conscious and contextual factors that determine the change in a
speaker’s typical utterance, over time.</li><br>
<li><i>Each state</i>: an artificial division of each word into temporal chunks.</li><br>
<li><i>Emitted observations</i>: a 12D feature vector; one frame of an utterance by \(A\).</li><br>
<li><i>Time unit</i>: one frame taken from one word by A (in our case, 10ms).</li><br>
<li><i>The whole model</i>: one speaker A’s ‘typical’ (word-independent) utterance pattern.</li>
</ul><br>

This adapts the (E25) approach for verifying a speaker, with one modification; since we know which speaker uttered each utterance, we can compare the likelihood $$P(\, O_i^{(A)} \,|\ \theta_A)$$ 
generated by the model of the actual speaker of to the likelihood $$P(\, O_i^{(A)} \,|\ \theta_B)$$
of their interlocutor’s model generating $$O_i^{(A)}$$. 

Call this the <i>point estimate of speaker distance</i> $$d_i^{(A)}$$ for an utterance by $$A$$:

$$
	d_i^{(A)} = \frac{ P(\, O_i^{(A)} \,|\ \theta_A) } { P(\, O_i^{(A)} \,|\, \theta_B	 ) }
	
	\qquad\qquad \text{(E26)}
$$

Where values of $$d_i^{(A)} > 1$$ represent model $$\theta_A$$ explaining $$O_i^{(A)}$$ better than $$\theta_B$$ does. 

The speaker-distance thus serves as a point estimate of the relative distance of $$A$$ from $$B$$ at the time of utterance $$t_i^{(A)}$$. This ratio is calculated for each utterance by $$A$$, and the point estimates are then correlated over time by Spearman’s rank correlation coefficient $$\rho$$ :


$$
	\rho \, \large(\,d_i^{(A)}, t_i^{(A)} \, \large) \,=\, 1 - \frac{6 \,\times\, \Sigma(d_i^{(A)})}  { N_k (N_k^2 - 1) }
	
	\qquad\qquad \text{(E27)}
$$

At last, then, we've derived our final target variable: the time correlation (E27) of the conditional word-likelihood ratios. If is
statistically significant, it is possible to say that A is converging to (for negative ), or diverging from
(for positive ) B’s characteristic speech pattern.<br><br><br>

<center>
<img src="/img/accommodation/9.png" width="100%" height="100%" />

<br><br><br>
<i>Fig.9</i> - Illustration of the accommodation analysis conducted for each task k of each speaker A.
</center>
<br><br>

<br>
Figure 9 illustrates the process of obtaining transcriptions and inferring the presence of
accommodation for each of speaker A’s units of analysis (tasks, k). This is repeated for each of A’s 12 tasks, for each other speaker, and then inferences about the presence and distribution of accommodation are made (see section 7).

<br><br><br><br>

<hr /><br>