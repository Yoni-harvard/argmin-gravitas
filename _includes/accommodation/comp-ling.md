HMMs were originally used for automatic word recognition (i.e. for modelling dictionary words and outputting the most likely word or sentence given an input vector).<a href="#fn:41" id="fnref:41">41</a> Since the present work is instead focussed on speaker recognition, we cover the use of HMMs in speaker
verification, only introducing the statistical machinery in common. 

For example, in word-recognition applications, the point of multiple HMM states is to ensure that only certain patterns of phonemes
or words are accepted (representing the constraints of the morphology of the language, or the
grammar). In our case the states capture, instead, distributions of acoustic evidence over time:
roughly, the speaker’s overall change over the course of pronouncing a word.

A primary motivator for using HMMs in linguistics is that the models are doubly stochastic, i.e. they are
capable of modelling simultaneously two domains of speech, like time and frequency. 

The Markov PFSM is our representation of temporal changes in speech (for instance changes in speech rate or
intonation); the emission GMM of each state represents the variation in the spectral properties of a given linguistic token.

In general, we use HMMs to model linguistic phenomena by building a recogniser for each desired
phone (isolated linguistic unit), as seen in Fig.8. A recogniser consists in a trained set of 'phone
models', a series of unlabelled novel inputs (the test set) and some way of categorising the model’s
output according to the most likely unit (the transcriptions). The recogniser is, then, a map function
for mapping speech (represented as sequences of vectors) to symbol sequences.


<center>
<img src="/img/accommodation/8.png" width="40%" height="40%" />

<br><br>	
<i>Fig.8</i> - The general form of a HMM recogniser. Here, 3 HMMs are trained and checked. The diagram is agnostic to the type of entity being recognised (words, phonemes, or speakers). <br>(The speech input we use is defined in 4.1 and 5.2 below.)
</center>
<br><br>	





This general recognition task is not easy. The mapping from linguistic symbols to speech waves is many-to-one; i.e. different symbols can easily give rise to similar speech sounds (consider
“foxhole” and “Vauxhall”, or “Lady Mondegreen” and “laid him on the green”). 

And the statistical noise between two waveform tokens of the same symbolic unit is often very great, due to variations
between speakers, between context (speaker mood, social cues) and paralinguistic effects (pitch,
intonation, speech rate). These problems make parameter optimisation and probabilistic judgment (e.g. offering multiple scored hypotheses) so key to recogniser performance.
<br><br>

<h4>Assumptions</h4>

We can only use HMMs for linguistic purposes given some key assumptions, including two important approximations to the speech signal:<br><br>

<ol>
	<li><i>Stationarity</i>: In order to have distinct training and testing phases for our models, we assume
that a speech signal can be well-approximated as a concatenation of static distributions
(more formally, as a ‘piecewise stationary process’, a stochastic process whose joint
probability distribution does not change over time). This requires us to first ‘window’ the
signal into discrete and periodic units, on the order of 10ms long: we assume stationary of
the waveform over this short duration (see section 5.2).</li><br>

	<li><i>Markov process</i>: that the sequence of observed speech vectors corresponding to
each word is generated by a Markov model (that each state depends only on the most
recent antecedent); and, as a corollary, that the information lost by making the Markov
assumption for speech does not occlude important patterns in the speech data, at least for
many stochastic purposes. (That is, we assume that the full, delicate N-gram chains of real
contextualised natural language are not the only property of interest.)</li><br>

	<li><i>Representativeness</i>: that the training data that will determine the model parameters is
properly representative of the linguistic unit being modelled.</li><br>

	<li><i>Unidirectionality</i>. That the only possible transitions are forward in time, since speech is a
temporal sequence. (i.e. Left-Right models.)</li>

</ol>
<br><br>

<h4>4.1. HMMs for speaker verification</h4>
<br>

Using HMMs to authenticate "voice-prints" involves similar machinery to the
speech recognition case, but uses speakers as the ‘words’ to be recognised. The models can quite
reliably distinguish speakers through the use of characteristic differences between the speech of
different people (inter-speaker variability). The general speaker-verification problem is simply:

> Given a segment of speech, [$$O$$], and a hypothesized speaker, [$$A$$], the task of speaker detection... is to determine if [$$O$$] was spoken by [$$A$$].<a href="#fn:43" id="fnref:43">43</a>


A system for verifying speakers requires the following subsystems:<br><br>

<ol>
	<li>An acoustic analysis subsystem, which extracts many ‘features’ (vector measurements) from each speech signal at regular intervals (frames).</li><br>
	<li>A modelling subsystem, which creates a model of speaker A and estimates optimal parameters using training outputs from subsystem (1).</li><br>
	<li>A scoring subsystem, which determines how well each non-training output from (1) fits the model .</li>
	<li>A decision subsystem, which uses the output of (3) and a pre-specified precision threshold to authenticate or reject the speaker A.</li>
</ol><br>

The acoustic analysis step is covered in section 5.2.d below; the modelling step is as above (though
actually simple GMMs are the dominant approach for text-independent speaker verification).<a href="#fn:44" id="fnref:44">44</a>One
simple scoring measure is to evaluate the conditional probability (the likelihood) of an utterance
being generated by the model and to compare this . And a simple decision measure is the hypothesis
test between (“the utterance is not by speaker A”) and (“the utterance is not by speaker
A”):
(E26)
The ratio is then compared to a threshold value; if
exceeds it,
utterance by A; if it does not, the null hypothesis is not rejected.
is accepted as a genuine
This approach – taking the likelihood ratio of an utterance conditional on speaker models – was
adapted for use in detecting accommodation by the SSSV experiment, and which the present work
relies on, as we will see.<br><br>


<h4>4.2. HMMs for analysing accommodation</h4>

Finally we reach our method. In speaker verification modelling, each HMM is a series of statistical patterns representing the
‘acoustic realisation’ of a modelled speaker (i.e. a series of distributions summarising the speaker’s
characteristic speech vectors). Take the components of our speaker HMMs as representing the
following:


<ul>
<li>The hidden process: the non-conscious and contextual factors that determine the change in a
speaker’s typical utterance, over time.</li><br>
<li>Each state: an artificial division of each word into temporal chunks.</li>
<li>Emitted observations: a 12D feature vector; one frame of an utterance by A.</li>
<li>Time unit: one frame taken from one word by A (in our case, every 10ms).</li>
<li>The whole model: one speaker A’s ‘typical’ (word-independent) utterance pattern.</li>
</ul>

We can adapt the (E26) approach for detecting accommodation with one modification; we compare the likelihood generated by the model of the actual speaker of
to the likelihood of their interlocutor’s model generating . 

Call this the <i>point estimate of speaker distance</i> for an utterance by A, :
(E27)

Where values of > 1 represent the situation in which model explains better than model does. 

The speaker-distance thus serves as a point estimate of the relative distance of A from B at . This ratio is calculated for each utterance by A, and the point estimates are then correlated over time by Spearman’s rank correlation coefficient, :

∑( ) (E28) 

At last, then, we have our target variable: the correlation (E28) of these likelihood ratios. If is
statistically significant, it is possible to say that A is converging to (for negative ), or diverging from
(for positive ) B’s characteristic speech pattern.

<center>
<img src="/img/accommodation/9.png" width="100%" height="100%" />

<br><br>	
<i>Fig.9</i> - Illustration of the accommodation analysis conducted for each task k of each speaker A.
</center>
<br><br>

<br><br>

Figure 9 illustrates the process of obtaining transcriptions and inferring the presence of
accommodation for each of speaker A’s units of analysis (tasks, k):

f) each word in k, (

g) the likelihood ratio ) is conditionalised on and then on calculated;

h) lastly the set of likelihood ratios are paired with the time the word i was spoken to
calculate .

This computation is repeated for each of A’s tasks , for each other speaker, and inferences about the presence and distribution of accommodation are made (see section 7).

<br><br><br><br>

<hr /><br>