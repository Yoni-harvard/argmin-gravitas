---
layout:     post
title:      "Effective Altruism Global: x: Oxford"
baselink:   /eagox
permalink:  /eagox
date:       2016-11-21
author:     Gavin   
img:        

visible:    1
published:  true

summary:    Assorted notes from my first meetup with the analytical do-gooders.
quality:    5
confidence: 7
importance: 8
emotion: 	7
warnings: 	
wordcount:  1600
categories: effective-altruism, philosophy
argument:	
---

<div class="separator" style="clear: both; text-align: center;">
	<img src="/img/eagox/thiel.jfif" width="60%" />
</div>

{% assign mood = "https://lukemuehlhauser.com/musks-non-missing-mood/"		%}
{% assign nate = "https://forum.effectivealtruism.org/posts/hkimyETEo76hJ6NpW/on-caring"	%}
{% assign ocb = "https://www.youtube.com/watch?v=67oL0ANDh5Y#action=share"		%}
{% assign parfit = "https://www.youtube.com/watch?v=KtU0pah4R8Q"		%}
{% assign anders = "https://www.youtube.com/watch?v=kt_8nLrUkI8"		%}

<br>

I'm not a joiner. <a href="#fn:1" id="fnref:1">1</a> But I have a lot of strange ideas, and a lot of odd energy, and a lot of <a href="{{mood}}">unusual</a> <a href="{{nate}}">feelings</a>, and these usually mislead people who go off on their own. So it's a stroke of incredible fortune that a movement of people with these things happens to arise - just as I graduate and try to become technical enough to understand what the best thing to do is.

I'm not sure I've ever experienced this level of background understanding, these tiny inferential distances, in a large group. Deep context - years of realisations - mutually taken for granted; and so shortcuts and quicksteps to the frontier of common knowledge. In none of these rooms was I remotely the smartest person. An incredible feeling: you want to start lifting much heavier things as soon as possible.


<!-- This is about <a href="http://eagxoxford.com/">this thing</a>.<br /> -->




<br>
One liners:
<br>
<blockquote>Effective altruism is to the pursuit of the good as science is to the pursuit of the truth.</blockquote> (Toby Ord)
<br>
<blockquote>If the richest gave just the interest on their wealth for a year they could double the income of the poorest billion.</blockquote> (Will MacAskill)
<br>
<blockquote>If you use a computer the size of the sun to beat a human at chess, either you are confused about programming or chess.</blockquote> (Nate Soares)
<br>
<blockquote>Evolution optimised very, very hard for one goal - genetic fitness - and produced an AGI with a very different goal: roughly, fun.</blockquote>
(Nate Soares)
<br>
<blockquote>The goodness of outcomes <b>cannot</b> depend on other possible outcomes. You're thinking of optimality.</blockquote> (Derek Parfit)

<br>

---

<br>

<div class="separator" style="clear: both; text-align: center;">
	<img src="/img/eagox/ai.png" />
	Soares, Ord, Krakovna, Shanahan, Hassabis, MacAulay. 
</div>

<br>

---

<br>

## <a href="{{ocb}}">Prospecting for Gold</a>

<div class="separator" style="clear: both; text-align: center;">
	<img src="/img/eagox/owen.jfif" />
</div><br>

<a href="https://www.fhi.ox.ac.uk/team/owen-cotton-barratt/">Owen Cotton-Barratt</a> formally restated <i>the</i> key EA idea: that importance has a highly heavy-tailed distribution. This is a generalisation from the GiveWell/OpenPhil research programme, which dismisses (ahem, "fails to recommend") almost everyone because a handful of organisations are thousands of times more efficient at harvesting importance (in the form of unmalarial children or untortured pigs or <a href="http://www.existential-risk.org/">an unended world</a>). 


<br>
<i>Then</i>, Sandberg's big talk on power laws generalised on Cotton-Barratt's, by claiming to find the mechanism which generates <i>that</i> importance distribution (roughly: "many morally important things in the world, from disease to natural disasters to info breaches to democides all fall under <a href="https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution">a single power-law-outputting distribution</a>"). 


<br>
Cotton-Barratt <i>then</i> formalised the Impact-Tractability-Neglectedness model, as a piece of a full quantitative model of cause prioritisation.
<br>
<div class="separator" style="clear: both; text-align: center;">
	<img src="/img/eagox/ocb.png" width="100%" />
</div>

<br>
<i>Then</i>, Stefan Schubert's talk on the <a href="https://books.google.co.uk/books?id=5Cs0CgAAQBAJ&amp;pg=PA303&amp;lpg=PA303&amp;dq=younger-sibling+syndrome+jon+elster&amp;source=bl&amp;ots=3Yiw0jqeW1&amp;sig=rqkeEP47CwHjttRu-X3r3YV6X_w&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwix_emF1LrQAhUBAsAKHUAgCOwQ6AEIGzAA#v=onepage&amp;q=younger-sibling%20syndrome%20jon%20elster&amp;f=false">younger-sibling fallacy</a> attempted to extend said <a href="https://causeprioritization.org/Importance,%20tractability,%20and%20neglectedness">ITN model</a> with a fourth key factor: awareness of likely herding behaviour and market distortions (or "diachronic reflexivity"). 


<br>
There will come a time - probably now - when the ITN model will have to split in two: into one rigorous model with nonlinearities and market dynamism, and a heuristic version. (The latter won't need to foreground dynamical concerns unless you are 1) incredibly influential or 2) incredibly influenceable in the same direction as everyone else. Contrarianism ftw.)

<br>

---

<br>

What is the comparative advantage of us 2016 people, relative to future do-gooders?

<ul>
	<li>Anything happening soon. (AI risk)</li>
	<li>Anything with a positive multiplier. (schistosomiasis, malaria, cause-building)</li>
	<li>Anything that is hurting now. (meat industry)</li>
</ul>


<br>

---

<br>

## Sandberg: <a href="{{anders}}">one-man conference</a>

<div class="separator" style="clear: both; text-align: center;">
	<img src="/img/eagox/riis.png" width="100%" />
</div>

<br>
Anders Sandberg contributed to _six_ events, sprinkling the whole thing with his hyper-literate, uncliched themes. People persisted in asking him things on the order of "whether GTA characters are morally relevant yet". But even these he handled with rigorous levity.

My favourite was his take on the possible value space of later humans: "chimps like bananas and sex. Humans like bananas, and sex, <i>and</i> philosophy and competitive sport. There is a part of value space completely invisible to the chimp. So it is likely that there is this <i>other</i> thing, which is like <i>whoooaa</i> to the posthuman, but which we do not see the value in."


<br>

---

<br>

* Books usually say that "modern aid" started in '49, when <a href="https://en.wikipedia.org/wiki/Point_Four_Program">Truman announced</a> a secular international development programme. Really liked Alena Stern's rebuke to this, pointing out that the field didn't even <i>try</i> to be scientific until the mid-90s, and did a correspondly low amount of good, health aside. It didn't deserve the word, and mostly still doesn't.

* Nate Soares is an excellent public communicator: he broadcasts seriousness without pretension, strong weird claims without arrogance. What a catch.

* Dinner with <a href="https://www.facebook.com/robert.wiblin">Wiblin</a>. My partner noted that I looked flushed. I mean, I <i>was</i> eating jalfrezi.

* <a href="http://www.crassh.cam.ac.uk/people/profile/catherine-rhodes">Catherine Rhodes</a>' biorisk talk made me update in the worst direction: I came away convinced that biorisk is both extremely neglected and extremely intractable to anyone outside the international bureaucracy / national security / life sciences clique. Also that "we have no surge capacity in healthcare. The NHS runs at 98% of max on an ordinary day." This harsh blow was mollified a bit by news of Microsoft's <a href="https://www.microsoft.com/en-us/research/project/project-premonition/">mosquito-hunting drones</a> (for cheap and large-sample disease monitoring, not revenge).
<br>

---

<br>



## Inequality vs impact


Most sessions I attended had someone asking the same desultory question: "how might this affect inequality?" (AI, human augmentation, cause prioritisation as a priority.) The answer's always the same: if it can be automated and mass-produced with the usual industrial speed, it won't. If it can't, it will. 

Actually it was good to ask (and ask, and ask) this for an ulterior reason:

<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2726330">Molly Crockett's research</a> - how a majority of people <a href="#fn:2" id="fnref:2">2</a> might relatively dislike utilitarians - was great and sad. Concrete proposals though: people distrust people who don't appear morally conflicted, who use physical harm for greater good, or more generally who use people as a means. So express confusion and regret, support autonomy whenever the harms aren't too massive to ignore, and put extra effort into maintaining relationships.

These are pretty superficial. Which is good news: we can still do the right thing (and profess the right thing), we just have to present it better. 

(That said, the observed effects on trust weren't that large: about 20%, stable across various measures of trust.)

<br>
<div class="separator" style="clear: both; text-align: center;">
	<img src="/img/eagox/crockett.png" />
</div>


<br>

---

<br>

## <a href="{{parfit}}">The Last Dance of Derek Parfit</a>

<div class="separator" style="clear: both; text-align: center;">
	<img src="/img/eagox/Capturesmol.png" />
</div>
<br>

Very big difference in style and method between Parfit's talk and basically all the others. This led to a sadly fruitless Q&amp;A, people talking past each other by bad choice of examples. Still riveting: emphatic and authoritative though hunched over with age. Big gash on his face from a fall. A wonderful performance. <a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/LastOfHisKind">Last of His Kind</a>.

Parfit handled 'the nonidentity problem' (how can we explain the wrongness of situations involving merely potential people? Why is it bad for a species to cease procreating?) and 'the triviality problem' (how exactly do tiny harms committed by a huge aggregate of people combine to form wrongness? Why is it wrong to discount one's own carbon emissions when considering the misery of future lives?).

<div class="separator" style="clear: both; text-align: center;">
	<img src="/img/eagox/handout.jfif" width="70%" />
</div><br>

He proceeded in the (lC20th) classic mode: state clean principles that summarise an opposing view, and then find devastating counterexamples to them. All well and good as far as it goes. But the new principles he sets upon the rubble - <a href="https://www.amazon.com/What-Matters-Three-Derek-Parfit/dp/0198778600">unpublished so far</a> - are sure to have their own counterexamples in production by the grad mill.

The audience struggled through the fairly short deductive chains, possibly just out of unfamiliarity with philosophy's unlikely <a href="https://en.wikipedia.org/wiki/Apodicticity">apodicticity</a>. They couldn't parse it fast enough to answer a yes/no poll at the end. ("_Are you convinced of the non-difference view?_")<br><br>

The Q&amp;A questions all had a good core, but none hit home for various reasons:


<blockquote>Does your theory imply that it is acceptable to <a href="http://lesswrong.com/lw/kn/torture_vs_dust_specks/">torture one person to prevent a billion people</a> getting a speck in their eye?</blockquote> 

Parfit didn't bite, simply noting, correctly, that 1) <a href="http://www.thedailyphilosopher.org/daily/000015.php">Dostoevsky said this</a> in a more manipulative way, and 2) it is irrelevant to the Triviality Problem as he stated it. (This rebuffing did not appear to be a clever PR decision - though it was, since he is indeed a <a href="https://en.wikipedia.org/wiki/Average_and_total_utilitarianism#Total_utilitarianism">totalarian</a>.)</li>

<br>

<blockquote>Sandberg: What implications does this have for software design?</blockquote> 

Initial response was just a frowning stare. (Sandberg meant: lost time is clearly a harm; thus the designers of mass-market products are responsible for thousands of years of life when they fail to optimise away even 1 second delays.)


<blockquote>I'd rather give one person a year of life than a million people one second. Isn't continuity important in experiencing value?</blockquote> 

This person's point was that Parfit was assuming the linearity of marginal life without justification, but this good point got lost in the forum. Parfit replied simply - as if the questioner was making a simple mistake: "These things add up". I disagree with the questioner about any such extreme nonlinearity - they may be allowing the narrative salience of a single life to distract them from the sheer scale of the number of recipients in the other case - but it's certainly worth asking.


<br>
We owe Parfit a lot. His emphasis on total impartiality, the counterintuitive additivity of the good, and most of all <a href="http://fas-philosophy.rutgers.edu/chang/papers/onwhatmatters.pdf">his attempted cleaving</a> of old, fossilised disagreements to get to the co-operative core of diverse viewpoints: all of these shine throughout EA. I don't know if that's coincidental rather than formative debt. <br><br>

(Other bits are not core to EA but are still indispensable for anyone trying to be a consistent, <a href="https://en.wikipedia.org/wiki/Mere_addition_paradox">non-repugnant</a> consequentialist: e.g. thinking in terms of degrees of personhood, and what he calls "lexical superiority" for some reason (it is two-level consequentialism).) 

The discourse has diverged from non-probabilistic apriorism, also known as philosophy, <a href="https://en.wikipedia.org/wiki/Great_Conversation">the Great Conversation</a>. Sandberg is the new kind of philosopher: a scientific mind, procuring probabilities, but also unable to restrain creativity/speculation because of the heavy, heavy tails here and just around the corner.



<div class="separator" style="clear: both; text-align: center;">
	<img src="/img/eagox/dark.jpg" width="100%" />
</div>
<br>

Incredibly beautiful setting (Exam School). Incredibly professionally organised by undergraduates, chiefly Oliver Habryka and Ben Pace.

<br><br>

{%	include eagox/foots.md	%}
