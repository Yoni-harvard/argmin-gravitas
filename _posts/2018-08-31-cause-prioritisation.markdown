---
layout:     post
title:      "What is best?"
baselink:   /prioritisation
permalink:  /prioritisation
date:       2019-08-23
author:     Gavin

img:        /img/
published:	false
visible: 	false
quality: 	7

summary:    Current shoddy opinion on the most important goals, axiology.
confidence:	50%
categories: 
importance: 10
wordcount:		
---

> Whatever the Theosophical cafés and Kantian bistros say, we're deplorably ignorant of the nature of the good. 

<center>– Marcel Proust <a href="#fn:1" id="fnref:1">1</a></center>


> Not to do good, because I don't know what good is, nor even if I do it when I think I do. How do I know what evils I generate if I give a beggar money? How do I know what evils I produce if I teach or instruct?

- Pessoa

<br>

Here's a story. You notice that, though poverty is falling, is scarcer than it has ever been, that many people still live lives of abject poverty. This is clearly bad; everyone agrees. You set out to help however you can. 

Say then that you notice that one of the first things people do when they rise out of poverty is to increase their meat consumption, and industrialise (that is, torture) their animals.

WAS net-negative

Climate catastrophe

AI slowdown. 

(I don't endorse this model, or maybe with <5% credence, it's just an illustrative example.)

It's not just that what you were doing before is less good than you thought; at each step <i>the sign</i> of the value of your action flips entirely. Up is down, altruism is harm. 



https://forum.effectivealtruism.org/posts/NQR5x3rEQrgQHeevm/what-new-ea-project-or-org-would-you-like-to-see-created-in#too3tJZWC74jzhiSj



1. Cause-impartiality: to select causes based on impartial estimates of impact.
2. Cause-agnosticism: to be uncertain about which cause is highest impact.
3. Cause-divergence: investments in multiple causes. 
4. Cause-generality: doing things that can affect any cause

Moral impartiality


If Earth dies, the entire universe dies?



#### Why x-risk and not cause prioritisation?

Pragmatic Argument: Only two philosophies imply that the end of the world and the prevention of the future are not terrible:

Nihilism (value is not real)
Negative utilitarianism ()


### Top 10 causes

1. Risks from advanced AI
2. Risks from biological weapons
3. Risks of fundamental moral error
4. Risks from
5. 
6. Risks from catastrophic climate change?
7. 
8. 
9. 
10.
11. Ending death

### Cause scepticism

Naturalism
-> Anti-real consequentialism
-> EV
-> X-risk
-> AI safety


The goal is maxipok, "no unalignment".



### Ideas

* 
* Free PIGD for every new parent in the world.

<br><br>

_I owe most of the ideas in this to Anders Sandberg, Toby Ord, and James Dama._



{%  include comments.html %}


<div class="footnotes">
<ol>

	<li class="footnote" id="fn:1">
		Que les cafés théosophiques et les brasseries kantiennes en prennent leur parti, nous ignorons déplorablement la nature du Bien. 
	</li>

</ol>
</div>