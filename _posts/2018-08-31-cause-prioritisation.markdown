---
layout:     post
title:      "What is best?"
baselink:   /prioritisation
permalink:  /prioritisation
date:       2018-08-23
author:     Gavin

img:        /img/
published:	false
visible: 	false

summary:    
confidence:	30%.
categories: 
importance: 10.
count:		
---


> Whatever the Theosophical cafés and Kantian bistros say, we're deplorably ignorant of the nature of Good. 

<center>- Marcel Proust <a href="#fn:1" id="fnref:1">1</a></center>

<br>

Here's a story. You notice that, though poverty is falling, is scarcer than it has ever been, that many people still live lives of abject poverty. This is clearly bad; everyone agrees. So you set out to help however you can. 

Say then that you notice that one of the first things people do when they rise out of poverty is to increase their meat consumption, and industrialise (that is, torture) their animals.

WAS net-negative

Climate catastrophe

AI slowdown. 

(I don't endorse this model, or maybe with 10% credence, it's just an illustrative example.)

It's not just that what you were doing before is less good than you thought; at each step <i>the sign</i> of the value of your action flips entirely. Up is down, altruism is harm. 

I owe this point to James Dama.


1. Cause-impartiality: to select causes based on impartial estimates of impact.
2. Cause-agnosticism: to be uncertain about which cause is highest impact.
3. Cause-divergence: investments in multiple causes. 
4. Cause-generality: doing things that can affect any cause

Moral impartiality


If Earth dies, the entire universe dies?



#### Why x-risk and not cause prioritisation?

Pragmatic Argument: Only two philosophies imply that the end of the world and the prevention of the future are not terrible:

Nihilism (value is not real)
Negative utilitarianism ()


### Top 10 causes

1. Risks from advanced AI
2. Risks from biological weapons
3. Risks of fundamental moral error
4. Risks from
5. 
6. Risks from catastrophic climate change?
7. 
8. 
9. 
10.
11. Ending death

### Cause scepticism

Naturalism
-> Anti-real consequentialism
-> EV
-> X-risk
-> AI safety


The goal is maxipok, "no unalignment".


<br><br>

{%  include comments.html %}


<div class="footnotes">
<ol>

	<li class="footnote" id="fn:1">
		Que les cafés théosophiques et les brasseries kantiennes en prennent leur parti, nous ignorons déplorablement la nature du Bien. 
	</li>

</ol>
</div>