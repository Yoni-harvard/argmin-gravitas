---
layout:     post
title:      "Neither Turing neither Searle"
baselink:   /safety-papers
permalink:  /safety-papers
date:       2018-02-28  <!--site.time-->
author:     Gavin

img:        /img/
published:	false
visible: 	0

summary:    Elementary discussion of philosophy of mind with one original argument.
confidence:	90% (because negative conclusion).
categories: AI
warnings:	
count:		
---
                                                                         
Q: ‘Some computer programs might be able to pass a Turing test, but that doesn’t provide any evidence that they can think. They might use all the right words, but that doesn’t suggest that they understand what the words mean.’

The Turing test is supposed to be a sufficient condition of thought, or at least evidence for thought. Intended to sidestep the question ‘Can machines think?’ which he deemed “too meaningless for discussion.”1 The replacement question the Test raises is characterised by the Stanford Encyclopaedia as:

Is it theoretically possible for a finite state digital computer, provided with a large but finite table of instructions, or program, to provide responses to questions that would fool an unknowing interrogator into thinking it is a human being? 2 

Despite Turing’s attitude to the original query, I intend to discuss it, his substitute and the position stated in the question which contests the Test so vehemently. The latter’s refutation of the epistemological worth of the Turing Test could have a basis in either or both of: the perceived critical flaws in the design of the Test (as it is commonly exegesised by contemporary philosophers), or in the implied philosophical doctrines to which Turing’s thought belongs, which I get somewhat intense about in the second half of the essay.

One design criticism is that the test is flatly subjective: since there is just one human observer on which the result relies, no even passably objective data can be derived from any outcome. This interrogator-dependence, when coupled with the vast potential variance in the performance of the human interrogatee, diminishes the Test’s potential value as part of an empirical case for intelligence, and makes its metaphysical claim to sufficient condition rather less concrete. A related criticism, from Ned Block’s ‘The Mind As Software of the Brain’ is that if we were to vet the interrogators to only those people philosophically- and/or computer-theory-capable with a view to increasing the reliability of their judgement, this specification would limit the Test to only defining intelligence in preconceived, ‘mentalistic’ terms. (It is interesting to note that the test requires the interrogator to be informed that one of A or B is in fact a machine; “ordinary” exchanges where this has not been delineated are not considered valid, perhaps because our mannered tolerance of idiosyncrasy from other humans would nearly preclude identification otherwise.) 

Block also provides a thought experiment to counter the Test’s absolute, metaphysical sufficiency in the “Blockheads,” machines that have preprogrammed responses to any possible input within a given timeframe. He argues that these machines would have only ‘the intelligence of a juke-box’3 but would pass the Turing Test easily. It is a matter of some contention that while Blockheads are logically conceivable, the case may well be that they are a physical impossibility and thus not a credible gap in the Test’s integrity. Issues such as memory capacity and time taken to complete the preprogramming seemed insurmountable until recently, with technologies such as hard drives and generative grammar arising; however, going by the contemporary standard of chatbots (weak AI programs in a similar vein) available for interview4 5 6 the true Blockhead remains an abstract. 

An <i>operational</i> (that is, effective or simulated) intelligence of machines, Turing committed necessarily to a functionalist view of the mind; the Turing Test is thus open to the criticisms thereof. The above question-viewpoint carries an implied objection to functionalism in its appeal to “understanding”; where functionalist thought operates in lieu of such metaphysical considerations, ascribing mental states instead by output or appearance, the question assumes a position analogous to that of John R Searle et al – that is, arguing for the description of consciousness as a physical property of brains and also for the necessity of intentionality for a mind. Searle states (and states, and states) that: 

<blockquote>
...the presence of a program at any level which satisfies the Turing test is not sufficient for, nor constitutive of, the presence of intentional content. [Jacquette] thinks that I am claiming “Program implies necessarily not mind” whereas what I am in fact claiming is “It is not the case that (necessarily (program implies mind)).”
</blockquote>7 

His “Chinese Room” argument (a “semantic engine” metaphor for a computer system, in which no component of the system understands Chinese, but the Room can translate it nonetheless) raises a number of problems for functionalism and thus also for the Turing Test.

For example, he argues, reasonably, that syntax (the manipulation of symbols) does not entail semantics (the understanding of the symbols). Cole represents Searle’s syntax argument as: 

    1. Programs are purely formal (syntactic). 
    2. Human minds have mental content (semantics). 
    3. Syntax by itself is neither constitutive of, nor sufficient for, semantic content. 
    4. Therefore, programs by themselves are not constitutive of, nor sufficient for minds. 8

This is unsatisfying: computer systems (hardware + program) are not “purely formal”; they possess changeable internal states that alter according to the syntax of the program, something quite reminiscent of human representational theory of mind. The first theories of semantics generation have been tentatively conceived, most readably by Douglas Hofstadter and his cabal of cognitive scientists.9 Current, traditional symbol-manipulating computers demonstrably do not understand that which they manipulate. But to rule out the possibility of non-biological Understanding – from that of an emergent or semi-chaotic system, for example – is just dogmatism.

Searle is committing a sort of empathic fallacy, like Thomas Nagel's famous bat (1974)10; the Chinese Room (premise 3) argument assumes that syntax is not sufficient for semantics, despite the impossibility of experience of ‘being’ such a system to verify this assertion directly. Maybe I'm making an anthropomorphic  error, but I don’t in any case agree that semantics are a necessary condition of what “intelligence” (as defined as the driver of, the capacity for problem solving, rationality or language use) is, or will be. If the simulation shares all external properties, we lose the basis for self-aggrandising.


1. Searle: "all purely syntactic systems lack subjective experiences."
2. Searle: "I have subjective experiences."
3. So Searle: "I am not a purely syntactic system." (modus tollens, 1&2)

4. The only system that Searle has knowledge about the subjective experiences of is himself.
5. So if Searle is not a purely syntactic system, he has no knowledge of what it is like to be a purely syntactic system,
6. He cannot therefore cannot assert premise 1. (5 & the knowledge account of assertion).
7. If Searle is a purely syntactic system, (1) is false. (by 2)

8. Therefore premise (1) is either unwarranted or false. (by 6&7)


(These days I wouldn't use infallibilist knowledge as the baseball bat I did; I'd go at him with probabilism instead. And I'd do something against Searle's odd dichotomy between representational machines who are 'pure' syntax vs those which are fully semantic. But it is pretty entertaining as it is.) 

Another counter-functionalist point raised by the Chinese Room is the assertion that beliefs, desires, and grasp of semantics are necessary conditions for consciousness; that there is an “original intentionality” to truly intelligent, conscious agents. I cannot help but see this view as limiting. Hofstadter puts forward emotion, motivation and consciousness are functions of non-intentional leanings, biases and calculations – that:

<blockquote>
...eventually, when you put enough feelingless calculations together in a huge coordinated organization, you’ll get something that has properties on another level (...) When things get complicated enough, you’re forced to change your level of description.11
</blockquote>

I suppose that in the face of currently insoluble topics such as qualia or the rules governing semantic association I would press for a new concept of thinking – perhaps defining “intelligence” and “consciousness/mind” separately. Until we are apt to deduce the patterns that give rise to consciousness or the Self, perhaps “intelligence” could signify some other state, one more syntactic or neutral. But it is here that the ‘Can machines think?’ contention ensnares, as I begin to press language into another shape by fiat, for convenience.

The argument can be levelled that the Turing Test is in any case chauvinistic or anthropocentric, since the capacity for thought surely cannot rely on the capacity to trick a human interrogator with language. This is reasonable, but the test implies no necessary claim; Turing himself phrases the objection as: 
May not machines carry out something which ought to be described as thinking but which is very different from what a man does? This objection is a very strong one, but at least we can say that if nevertheless, a machine can be constructed to play the imitation game satisfactorily, we need not be troubled by the objection.12 

It is in any case difficult to suggest what non-linguistic, non-human-modelled intelligence a computer that humans create could possess. If we accept, say, Davidson’s psychological holism argument against animal consciousness, we encounter the same counter-functionalist, metaphysics-based arguments as Searle’s “original intentionality” objection, in that free-will, beliefs and language are characterised by them both as necessary conditions for a ‘mind’ in the full sense. The “Imitation Game” promotes only imitation (or, for the functionalist, a reproduction) of human intelligence, but it seems unreasonable to view this as a letdown. 

<blockquote>
	While there is logical force in the very real flaws in the Turing Test’s structure – from user error or the conceivable application of canned “brute force”, to the argument that it is an unnecessary limitation on what artificial intelligence might accomplish – I believe that as an elegant attempt to ground the question of computer minds empirically, it ought not be discounted. It seems that we anyway have no superior, scientific metric by which to begin probing the possibility of machine intelligence. I thus argue that while the passed Turing Test alone would not constitute grounds for rational belief in a successful machine’s consciousness, it would be a practical point of credibility. The metaphysical contention is the pivot of the matter; it is hard to discount Searle’s objection that the accepted conception of “mind”, as applied to humans or any other putative agent, centres on that most elusive input/output pattern, “intentionality”. But I dispute the designation of these distinctly human properties as necessary conditions within the general boundary of “intelligence.” 
</blockquote>




<div class="accordion">

<h3>Bibliography</h3>
<div>

	• Block, Ned (1995), ‘The Mind As Software of the Brain’; New York University website; http://www.nyu.edu/gsas/dept/philo/faculty/block/papers/msb.html 

	• Cole, David (2004); ‘The Chinese Room’; Stanford Encyclopadia of Philosophy; http://plato.stanford.edu/entries/chinese-room/ 

	• Hofstadter, Douglas (1981); ‘A Coffeehouse Conversation’, in D. Hofstadter & D. Dennett (eds.) The Mind's I, (London: Penguin), pp.69-92 

	• Hofstadter, Douglas (1995), Fluid Concepts & Creative Analogies (Bloomington; Basic)

	• Levin, Janet (2009); ‘Functionalism’; Stanford Encyclopaedia of Philosophy; http://plato.stanford.edu/entries/functionalism/#ThiMacTurTes 

	• Nagel, Thomas (1974); ‘What Is It Like To Be A Bat?’; The Philosophical Review LXXXIII, 4; pp.435-50

	• Oppy, Graham & Dowe, David (2008); ‘The Turing Test’, Stanford Encyclopaedia of Philosophy; http://plato.stanford.edu/entries/turing-test/

	• Searle, John R (1989); ‘Reply to Jacquette’, in Philosophy and Phenomenological Research, Vol. 49, No. 4, (Providence, International Phenomenological Society), pp. 701-708

	• Turing, Alan (1950); ‘Computing Machinery and Intelligence’, Mind, Vol. LIX, No.236 (Oxford; Oxford University Press), pp.53-67
</div>

</div>


